{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping: no known devices.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# tqdm().pandas()\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import cv2\n",
    "from os import listdir\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub  #pip install tensorflow_hub\n",
    "import os\n",
    "import tokenization \n",
    "from tqdm import tqdm_notebook\n",
    "from keras import backend as K\n",
    "# Initialize session\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "sess = tf.Session()\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) #command to run codeon multiple gpu\n",
    "\n",
    "\n",
    "\n",
    "# Params for bert model and tokenization\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "from keras_lr_finder import LRFinder\n",
    "import talos as ta\n",
    "from pprint import pprint\n",
    "import talos\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from random import choice\n",
    "import gc\n",
    "from talos.model.normalizers import lr_normalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1057393737372966912</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1057149180148240384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1113184475658293251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1108665597510991873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1108732814202228737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1035921045734211585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134817</th>\n",
       "      <td>1114657084849758209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134818</th>\n",
       "      <td>1107892812102668288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134819</th>\n",
       "      <td>1105670090098380806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134820</th>\n",
       "      <td>1108078550391894017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134821</th>\n",
       "      <td>1114353537755045890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134822 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1057393737372966912\n",
       "0       1057149180148240384\n",
       "1       1113184475658293251\n",
       "2       1108665597510991873\n",
       "3       1108732814202228737\n",
       "4       1035921045734211585\n",
       "...                     ...\n",
       "134817  1114657084849758209\n",
       "134818  1107892812102668288\n",
       "134819  1105670090098380806\n",
       "134820  1108078550391894017\n",
       "134821  1114353537755045890\n",
       "\n",
       "[134822 rows x 1 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_df(file):\n",
    "    return pd.read_csv(file,sep = '\\t')\n",
    "train_df = get_df('C:\\\\Users\\\\Dell\\\\Desktop\\\\ML\\\\archive\\\\splits\\\\train_ids.txt')\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1117460975903334410</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1043075096544509952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1115669983680454656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1035902082199572486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1036092005741256705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1037147555367280640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>1109972327897460736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1106756324803731456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1114945824851730434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1105122471743635456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1114810734872023040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9999 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1117460975903334410\n",
       "0     1043075096544509952\n",
       "1     1115669983680454656\n",
       "2     1035902082199572486\n",
       "3     1036092005741256705\n",
       "4     1037147555367280640\n",
       "...                   ...\n",
       "9994  1109972327897460736\n",
       "9995  1106756324803731456\n",
       "9996  1114945824851730434\n",
       "9997  1105122471743635456\n",
       "9998  1114810734872023040\n",
       "\n",
       "[9999 rows x 1 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = get_df('C:\\\\Users\\\\Dell\\\\Desktop\\\\ML\\\\archive\\\\splits\\\\test_ids.txt')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1062433411107713025</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1045809514740666370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1107854991774138368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1054203094546829313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1035252480215592966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1037047746593603584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>1114923019942514688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>1114673262427086849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>1113637700748419072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>1110005215003066380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>1105285622317674496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4999 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1062433411107713025\n",
       "0     1045809514740666370\n",
       "1     1107854991774138368\n",
       "2     1054203094546829313\n",
       "3     1035252480215592966\n",
       "4     1037047746593603584\n",
       "...                   ...\n",
       "4994  1114923019942514688\n",
       "4995  1114673262427086849\n",
       "4996  1113637700748419072\n",
       "4997  1110005215003066380\n",
       "4998  1105285622317674496\n",
       "\n",
       "[4999 rows x 1 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = get_df('C:\\\\Users\\\\Dell\\\\Desktop\\\\ML\\\\archive\\\\splits\\\\val_ids.txt')\n",
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Image URL</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Tweet URL</th>\n",
       "      <th>Tweet Text</th>\n",
       "      <th>Labels_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1114679353714016256</td>\n",
       "      <td>http://pbs.twimg.com/tweet_video_thumb/D3gi9MH...</td>\n",
       "      <td>[4, 1, 3]</td>\n",
       "      <td>https://twitter.com/user/status/11146793537140...</td>\n",
       "      <td>@FriskDontMiss Nigga https://t.co/cAsaLWEpue</td>\n",
       "      <td>[Religion, Racist, Homophobe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1063020048816660480</td>\n",
       "      <td>http://pbs.twimg.com/ext_tw_video_thumb/106301...</td>\n",
       "      <td>[5, 5, 5]</td>\n",
       "      <td>https://twitter.com/user/status/10630200488166...</td>\n",
       "      <td>My horses are retarded https://t.co/HYhqc6d5WN</td>\n",
       "      <td>[OtherHate, OtherHate, OtherHate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1108927368075374593</td>\n",
       "      <td>http://pbs.twimg.com/media/D2OzhzHUwAADQjd.jpg</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://twitter.com/user/status/11089273680753...</td>\n",
       "      <td>“NIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL S...</td>\n",
       "      <td>[NotHate, NotHate, NotHate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1114558534635618305</td>\n",
       "      <td>http://pbs.twimg.com/ext_tw_video_thumb/111401...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>https://twitter.com/user/status/11145585346356...</td>\n",
       "      <td>RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...</td>\n",
       "      <td>[Racist, NotHate, NotHate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1035252480215592966</td>\n",
       "      <td>http://pbs.twimg.com/media/Dl30pGIU8AAVGxO.jpg</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "      <td>https://twitter.com/user/status/10352524802155...</td>\n",
       "      <td>“EVERYbody calling you Nigger now!” https://t....</td>\n",
       "      <td>[Racist, NotHate, Racist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1106978219654303744</td>\n",
       "      <td>http://pbs.twimg.com/media/D1zG0qnX4AAw9SC.jpg</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://twitter.com/user/status/11069782196543...</td>\n",
       "      <td>“ real ass bitch give a fuck boutta nigga” htt...</td>\n",
       "      <td>[NotHate, NotHate, NotHate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1113920043568463874</td>\n",
       "      <td>http://pbs.twimg.com/media/D3VwYEKW4AYz4vk.jpg</td>\n",
       "      <td>[5, 1, 1]</td>\n",
       "      <td>https://twitter.com/user/status/11139200435684...</td>\n",
       "      <td>@WhiteHouse @realDonaldTrump Fuck ice. White s...</td>\n",
       "      <td>[OtherHate, Racist, Racist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1114588617693966336</td>\n",
       "      <td>http://pbs.twimg.com/media/D3fQcCCWAAIG8tO.jpg</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://twitter.com/user/status/11145886176939...</td>\n",
       "      <td>Day’s a cunt https://t.co/Ie6QZReHsw</td>\n",
       "      <td>[NotHate, NotHate, NotHate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1045809514740666370</td>\n",
       "      <td>http://pbs.twimg.com/media/DoN2KFmXcAAIT-Y.jpg</td>\n",
       "      <td>[3, 3, 0]</td>\n",
       "      <td>https://twitter.com/user/status/10458095147406...</td>\n",
       "      <td>#sissy faggot https://t.co/bm1nk8HcYO</td>\n",
       "      <td>[Homophobe, Homophobe, NotHate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1108178453910695936</td>\n",
       "      <td>http://pbs.twimg.com/ext_tw_video_thumb/110817...</td>\n",
       "      <td>[4, 0, 3]</td>\n",
       "      <td>https://twitter.com/user/status/11081784539106...</td>\n",
       "      <td>@Gloriko_ Nigga what? https://t.co/nOwIJtgtU1</td>\n",
       "      <td>[Religion, NotHate, Homophobe]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Tweet ID                                          Image URL  \\\n",
       "0  1114679353714016256  http://pbs.twimg.com/tweet_video_thumb/D3gi9MH...   \n",
       "1  1063020048816660480  http://pbs.twimg.com/ext_tw_video_thumb/106301...   \n",
       "2  1108927368075374593     http://pbs.twimg.com/media/D2OzhzHUwAADQjd.jpg   \n",
       "3  1114558534635618305  http://pbs.twimg.com/ext_tw_video_thumb/111401...   \n",
       "4  1035252480215592966     http://pbs.twimg.com/media/Dl30pGIU8AAVGxO.jpg   \n",
       "5  1106978219654303744     http://pbs.twimg.com/media/D1zG0qnX4AAw9SC.jpg   \n",
       "6  1113920043568463874     http://pbs.twimg.com/media/D3VwYEKW4AYz4vk.jpg   \n",
       "7  1114588617693966336     http://pbs.twimg.com/media/D3fQcCCWAAIG8tO.jpg   \n",
       "8  1045809514740666370     http://pbs.twimg.com/media/DoN2KFmXcAAIT-Y.jpg   \n",
       "9  1108178453910695936  http://pbs.twimg.com/ext_tw_video_thumb/110817...   \n",
       "\n",
       "      Labels                                          Tweet URL  \\\n",
       "0  [4, 1, 3]  https://twitter.com/user/status/11146793537140...   \n",
       "1  [5, 5, 5]  https://twitter.com/user/status/10630200488166...   \n",
       "2  [0, 0, 0]  https://twitter.com/user/status/11089273680753...   \n",
       "3  [1, 0, 0]  https://twitter.com/user/status/11145585346356...   \n",
       "4  [1, 0, 1]  https://twitter.com/user/status/10352524802155...   \n",
       "5  [0, 0, 0]  https://twitter.com/user/status/11069782196543...   \n",
       "6  [5, 1, 1]  https://twitter.com/user/status/11139200435684...   \n",
       "7  [0, 0, 0]  https://twitter.com/user/status/11145886176939...   \n",
       "8  [3, 3, 0]  https://twitter.com/user/status/10458095147406...   \n",
       "9  [4, 0, 3]  https://twitter.com/user/status/11081784539106...   \n",
       "\n",
       "                                          Tweet Text  \\\n",
       "0       @FriskDontMiss Nigga https://t.co/cAsaLWEpue   \n",
       "1     My horses are retarded https://t.co/HYhqc6d5WN   \n",
       "2  “NIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL S...   \n",
       "3  RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...   \n",
       "4  “EVERYbody calling you Nigger now!” https://t....   \n",
       "5  “ real ass bitch give a fuck boutta nigga” htt...   \n",
       "6  @WhiteHouse @realDonaldTrump Fuck ice. White s...   \n",
       "7               Day’s a cunt https://t.co/Ie6QZReHsw   \n",
       "8              #sissy faggot https://t.co/bm1nk8HcYO   \n",
       "9      @Gloriko_ Nigga what? https://t.co/nOwIJtgtU1   \n",
       "\n",
       "                          Labels_str  \n",
       "0      [Religion, Racist, Homophobe]  \n",
       "1  [OtherHate, OtherHate, OtherHate]  \n",
       "2        [NotHate, NotHate, NotHate]  \n",
       "3         [Racist, NotHate, NotHate]  \n",
       "4          [Racist, NotHate, Racist]  \n",
       "5        [NotHate, NotHate, NotHate]  \n",
       "6        [OtherHate, Racist, Racist]  \n",
       "7        [NotHate, NotHate, NotHate]  \n",
       "8    [Homophobe, Homophobe, NotHate]  \n",
       "9     [Religion, NotHate, Homophobe]  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the JSON data\n",
    "with open('archive\\MMHS150K_GT.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a list to store the data\n",
    "tweets_data = []\n",
    "\n",
    "# Iterate through each tweet in the data and append it to the list\n",
    "for tweet_id, tweet_info in data.items():\n",
    "    tweets_data.append({\n",
    "        'Tweet ID': tweet_id,\n",
    "        'Image URL': tweet_info['img_url'],\n",
    "        'Labels': tweet_info['labels'],\n",
    "        'Tweet URL': tweet_info['tweet_url'],\n",
    "        'Tweet Text': tweet_info['tweet_text'],\n",
    "        'Labels_str': tweet_info['labels_str']\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df = pd.DataFrame(tweets_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999, 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134822, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Now you can use the tokenizer to tokenize text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FullTokenizer' from 'tokenization' (c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tokenization\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Dell\\Desktop\\ML\\spotfake.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Dell/Desktop/ML/spotfake.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtokenization\u001b[39;00m \u001b[39mimport\u001b[39;00m FullTokenizer\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dell/Desktop/ML/spotfake.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPaddingInputExample\u001b[39;00m(\u001b[39mobject\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dell/Desktop/ML/spotfake.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fake example so the num input examples is a multiple of the batch size.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dell/Desktop/ML/spotfake.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m  When running eval/predict on the TPU, we need to pad the number of examples\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dell/Desktop/ML/spotfake.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m  to be a multiple of the batch size, because the TPU requires a fixed batch\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Desktop/ML/spotfake.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m  battches could cause silent errors.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Desktop/ML/spotfake.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'FullTokenizer' from 'tokenization' (c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tokenization\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from tokenization import FullTokenizer\n",
    "\n",
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels).reshape(-1, 1),\n",
    "    )\n",
    "\n",
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples\n",
    "\n",
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "\n",
    "# Convert data to InputExample format\n",
    "train_examples = convert_text_to_examples(train_text, trainY)\n",
    "test_examples = convert_text_to_examples(test_text, testY)\n",
    "\n",
    "# Convert to features\n",
    "(train_input_ids, train_input_masks, train_segment_ids, trainY \n",
    ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
    "(test_input_ids, test_input_masks, test_segment_ids, testY\n",
    ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
